{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1yWW3GHNxBA4c8tEVDNECz_mGei7YLmQt",
      "authorship_tag": "ABX9TyPkb/7ee7UAmwtOyVEHApq3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kappagl/DLExamen1Parte2/blob/main/Examen1p2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x57HTemZ9xpR"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/m3ex02-data.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "5tkeMcW14Zxx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Que configuracion tienes\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMd9Z4y44dTW",
        "outputId": "2756ad94-460b-464c-b2fd-e7aedd90b1a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el caso de CDataset no usamos ImageFolder de pytorch directo y tuvimos que abrir 1 a 1 las imagenes puesto que esperaria un acomodo de folder distintos como:\n",
        "\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAKAAAADFCAYAAADT9vnEAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAABhaVRYdFNuaXBNZXRhZGF0YQAAAAAAeyJjbGlwUG9pbnRzIjpbeyJ4IjowLCJ5IjowfSx7IngiOjE2MCwieSI6MH0seyJ4IjoxNjAsInkiOjE5N30seyJ4IjowLCJ5IjoxOTd9XX1JWj3+AAATKUlEQVR4Xu2dP0wbyxbGz3160i0sUYAUp4mikDRuQmPJzS1MQ4WE7CZSWkThIjRxi4RESxpSUCDaSDRYkahocHEbJBrSuAlQ0MSR4gLJ9Xtz5s/uzHi9ttfYY8P3k6ywtne9jr89Z2f3O2f+ev369f8IgED8R/8LQBAgQBCUv16+fIkUDCbO33//rf9yQQQEQYEAQVAgQBCUGRHgJh2cndHZt10q62fA82CwALcO6IzF0fM4ELIBYDxGGAWXafdbnQqtffqw19TPATAc/UbBjyBA9fzixTpdvz+jyht+rktXXz7Q7oV8g4qiG8t6gej2+zptH6m/yzsnVC/l1MJdg9Y/Hau/JZyaV+j6S4fWPhdJvuvhivY/7hIOgfli4pdhljfOaO33Pq2vr1PjLkfFqknQQkTvr+Xz8vH9Vrw3Tt/NvQ/y+f3Lrn7GZ5kqnxfpXK7foNuFIlW39Etg7nm8QYiIXiYyHv+4JVp6pQcUx7RtR7Wja7qlPL1a1csD4Wi6LbbCHNP1HVH+JYYqT4VHE+DtD1tk27RupcnNr/bgpSJiGgCKiV+G4XO8yptbapgUzGlUvwbAxAUoeejQjf5z8ysiIIiZuACbe+dy4FDXKXjt95UVAXkErZ6XI+E3FZWmv+IK43MBbhgwFeCGATMJBAiCAgGCoECAICgQIAgKBAiCAgGCoECAICgzL0DbyHCy8zxcMHz//LmUJ8y8AI8/KRND404/AZ4USMEgKHNzL5hTMTuunXIAtvqvdiyLPlv416ijywE4ldXokM5f1HWpgFsOoN7vu3O8coJEVKlA5zJPxVKOupcNahUqVFyw11WlCsUFuUJCKYH32dbrskyh0IqW+bvz/sf77m2b2O5mTLuzybO9F5wr1aNSAbb9L6/G51bSGsZ1KOxT/HIlpMc/8iDxGZap+OJclhjkShVZE8OlCIV/1NbLO1WiI+OB3KcrKlLNOoflz85fqv2SD0ecMVJ8S0Kc4j1xHU2Nin/0fsvHbIsvjaefgq1Sgea/LeouLNJbubRJKxxVjJP7okmth1Hs/iLanep1RfQ6jaKqorm3bQm5Sc1Wl3Iv1CcbcoVy6kCjbMSXJM43K+IbzD/DCVBEjRM9ElUPryZ4y6sd9kZwriXfG80O2vbEuKGOENzye/1pq2UqiBTa+jcpDmXA+15R5Z/m+JOKisYneeAXWi0UqSgOkG5LiFc/ZeBCrsbdMlX0uvN8deC/+t90LnbpQ1pa4hoQLwLY8Ei2b4oYtO2J0aT7P+I8SppgK/KZrkiJw6XfQYjzu89FIk6xOvrK87oX8k9Nk3Y/ammxWD8LEZJ1fsrnhOLv2ue6EGfTOm9VxP+nfC5ZFyKkuazXnv8UHKVUPjEfwe4vfvQ1p1Zl/dF/wPavWGA1LwI6XNxTW//pwAenLGM9od2+VYQqks8rMy5APrpVmuFRIA8onJQjIm+cirg4foSCJ/HjnltpzDweJ50d0ykPeDb0dj8vUsupe46/l3qoAYkf5STiO+5fEhVFhFT7FpcxqIeI4mJAMo/Rj3m+lnyZ9rjg3RpByucK1Bp4GQaMCiz5Pu8WVasPi/I/BfFcm+4hvqnxrIuSzAXemNm/oDuvPEJzIgCygxQMZhIIEAQFAgRBgQBBUCBAEBQIEAQFl2FS8YyfXg9raTCI7vEaM6paJ6mZe2yqfeuaUTXKcOoZVZksfbHlXZ0i5Xr6bvv7PZ1rn7gMMzLKZeIYP3+sRLapyLWsX1P3a9lKpr1/PV4/9h8au9cxbcv11L1rFh5vw74XbJ4zZtb6CEVKvG9nYj9bSXU0WwdCfO3IhLF/madKwAIoCLAP5Z01WubI4/S33tYi2aRqiejqKI5Kzb1DunpYphXxwyvja4HKtoNla0Vsr0XNkW/zsW1rhObs7Lxhp7aImPf6qRgRnVeXqXt5GkU81b/R29cpAgEmIqJXIZdoBpWsvqJ8zz1j9hdqg6t0V8f2fGbzvfjh+21vIMpyNZRbmy1cXsr1iWxiEt52jhbf6cUpAwFmgY0MVtthw81vY7ny07CdfrOgxD0+ar/suhhZXxIVN02f4RzR5oRWL/acuIrzCnsiGv+k2b/pz87j6AR9ktvmlONUj6lzq+hca9Bn9+Nnh7obyghrS+rtCxE1fytZchqulVRqa75T6fc0s8umTK+WxD+/1VLm/RbwqUJZ/J9wKUBdLHNF39XDmvhO6vVpg1FwH6Sw+xUEyQFKXP6pUGLn6jgl8Hg0zGWhPSWlEjXiJfugSHpOC67tvG8w8jtQ7yjYhT9vha4nPBLGKHhEjk+vxEDCLaXkaKxGwTxhTo6KW14qE+PVuDrOpOEarY2VfoVAONrdNUYS33AYsU9WfGkgAqbhpzrvmpqT/pOu1Zn1e15TP/yw1wHd04pBJG872ndxEMWnNMMU4T8O8AOCoCAFg5kEAgRBgQBBUCBAEBQIEAQFAgRBgQBBUCBAEBQIEAQFd0L6oG6z9fZ8js0Gk8a9pZbq8olu9Zl7u/4+WvtOnrtIYtw0Cbfxkm4xZgB3QjLQfSDHVDo1+B6yEILdQ/qQqrpHYOyycSz7cpZ5NklYXV8NsvvrLV2zKLmZKK/3XRYDCOHxNlwzwjjlAKMCAabRalG7VBVxIRmOknGfPruJJIuElzmimNeHbz28WVXuF9uAEPWc3qpK181h9FqTdo+uqKt7Rh//EMLy+kfLrl931xkcLyOWA2QAAkylSaeXeVnn4cOFP6Z7PUeLuCjJkBPL7BnUkeRhmdaGan7pNU/3KL/ME/25d1Oi7LCq6lHo6FrENf23RJUX9NveYEYoB8gABDgAdjbnRUp0//u5KEn8qBfxuZFdlGSIp3zQ3kCvS34WbNd1jN2m10vDdvrNxGOVAyQDS37aZzPcyrd6QuVVs01Dlzrj2NgHfK9+cN2JErL9zre0uBDvD6fhyoZKwzcy/Z5nSL8GrxzgkZn9LvnjbFv8SFEn+iQGfbbm+EebzqplceZloyvJovVZBPrPYej7vVQ0K8oI1vvNmr/aVC+8khE5+ma6Su/a7Aun4Y2KiMZCPJx+L7LLT0VQv5Lu8UAKHoajU7paKlBBL7IwZJqzUrOpI/YnrBkdcdBc8ECi4swdUt45UIMcFpdTKiCi/BZPCRHX+sb7Vxsz/YpBFGeIiZQDKCDAoeBzOBHzrAjHkbfxJ55oRnYbeITrZRKOjnJ6BjOCPqManerzSe6q0BCjczVjAHfJT2oDIkfDC7ne0S+nfl5Ppn8zS4A7Qo8/V18KGlBnPA64EA2mAi5Eg5kEAgRBgQBBUCBAEBQIEAQFAgRBgQBBUCBAEBRciJ5DnCbjXsOkWQUXokdEmU19k+mZc382FM29D9qDaE+CPZ9AgCkEs+Q/IyDANDJb8tVrPMW+nDJBv8ee4uFkZ1fb9Q9o0xgEZF2Hwl7P3/ZTAgJMZRxLPlGuVKc6T5mg06Vt38qVCtT5oqz6ldUO7X+J6zrYKFulQ7letG2rG+tTAgIcwDiWfOly1gMENXeIamwuuTvX9ipytiMRy9t2QZK/7hMClvwJWvKdeUEs9/XgSNa737xvTxFchukDCzvqbM8HwfsOXS0VdWF6UgG4+5yzvoe8jMKp+dONFJrc5k8+GBbpXBwAxAeV3aFfHijqNfvgiLfjHDIzCS7DjMNULfmaqPRSWe7jKP20gACHYrqWfDlFxJuKHgGLCNkSy/o1Kchv5jOFLM37rBH0PIEUDKYCUjCYSSBAEBQIEAQFAgRBgQBBUCBAEBQIEAQFAgRBmQ8B8r3QOb7aD/qDCAiCgltxYCr0uxU38wK0/X69Xr8aUatNxdKyrA5rUEW+1/b8ScuSqSDrmaI+YV4M+z1bthexd3p7d9ue1xA4zO29YHadsC29caefcMhRsWDs7BXpv5PW96hBd7q1ffOrEB+XNfLrvA3xXNRYnM87N/JCdGrd9e9C6LblXrxeYweM3jY/IL7RmftzwNjOfkvnvvkz1druTYdw0aSWNR0Bz61Bl4dxxGNPoG+5d6ZDAFkYToBmFBo93OKbqO2reXgz63AatV/narGIQdsei9g7Jx+O/V41A3enM+hS618lWJ4OgYuK4v3yLPJssbfb6E5wNqGnzNwMQnot7qpuwrez31g2dXn+mGJtt88vGfscs/fz0un5LODwfP2A/aztQoxrb7gIKT6Hs8XGTb5zpdrQ9bg8fwcYnRmPgEmjVBOpaGAEVBEvFt3t5RXlS/0jIOOMtPnUwq7Is6rm/BGw/RroZW4vw0wMLx1L5HMFanmXW8D4wJLv826xp9JMzipJbbqH+KbGs74T0puCkUYnBVIwCApSMJhJIEAQFAgQBAUCBEGBAEFQIEAQFFyGSUUZHiIXjDclQrLZVa2TNIl0bHB4m3iLURlaE24/ek05h8LchkyYxsHd7+lc+8RlmJFhIQjx/dGGVX78WHEajdcLrYQe0dzKrUu5QtmzZ7H/0Ni9eNZzXq8hfn4lPN6GbWg1z62v79MVFak+gt2L9+1M7GcrycS7dSDEFxtp9y/zVAloJYMA+2AaTpoez5KjbS0S7hFNdHUURyW7R7QyvhaobDtptlbE9lrUHPk2X5N2PwqhLhSpOoz5VUS+GpsxRMS810/FiOi8ukzdy9Mo4jX3zsW2vX2dIhBgIiJ6FXJuj2eb1VeU77ln3KT7P9rgKt3VOWeOkc334ofvt72BKPOscWunwkZZL+X6tH/Ze8HbztHiO704ZSDALLCR4aEjfjqX2BPop2E7/WZBiXt81H65rYVrXjP06YIu+Wmf3Y+fHepuqNoSW1Js4+/+VrLkNFwrqdTWfKfS72lml02ZXi2Jf36rpcz7LeBThbL4P+HWwnWx3L1siFOHNfGd1OvTBqPgPkhh97XY8wBljTqOb1CJXXXRj5d5NHz+ot7H3q9GvGnd9iVacG3nfYOR34EGTWbIn7dC1xMeCWMUPCKyUbg48a/ZBVQiGqtRMHfJzzklnjKVifFq3CXfpOEarY2VfoVAONrdNUYS33AYsU9WfGkgAqbhpzrvmpqT/pOu1Zn1e15TP/yw1wHd04pBJG872ndxEKUV208K+AFBUJCCwUwCAYKgQIAgKBAgCAoECIICAYKgQIAgKBAgCAoECIKCOyF9ULfZ7FtVvtlg0ri31FJdPtGtPnNv199Ha9/J7/jFGDdNwm28pFuMGcCdkAx0H8gxlU4NvocshJAXolO2/HU6pKruVRi7bBzLvpxDhU0SVtdXg+z+ekvXLMqjbbXed1kMIITH23DNCOOUA4wKBJhGq0XtUlXEhWQ4SsYtfE+sZpYsEl7miGJeH7718GZVuV9sA0Jzb1tF4q2qdN0cRq81afeIm7SvyO1zY03Sfxtk16+76wyOlxHLATIAAabSpNPLfGIjci78kX5BGSnsoiRDTiyzZ1BHkodlWrOtXX3xmqd7lF/mra6vmot7aovEKffz6FrENbt5uiov6Le9wYxQDpABCHAA7GzOi5To/vdzUZL4UaMO/Ryh4qIkQzTlg3iX9Aa+UP35x8F2XccokSi8NGyn30w8VjlAMrDkp302I0R2Xj2h8qrZpqFLnXFs7AO+Vz+47kQJ2X7nW1pciPeH03BlQ6XhG5l+zzOkX4NXDvDIDCdArrRKMy3yiW3KEcaTzfT9D5jktsWPtPsx5Scd9Nma4x9tOquWxZmXja4ki9ZnEeg/h6Hv91LRrCgjWO83a/5qU73wSkbk6JvpKr1rsy+chjcqIhoL8XD6vcguPxVB/Uq6xwMpeBh4kpqlAhX0IgtDpjkrNZs64tiSnxVx0FzwQKISFcEz5Z0DNchhcTmlAiLKbxWJrFrfeP9qY6ZfMYjiDDGRcgAFBDgUfA4nYp4V4TjyNv4UZXUZj3Jlt4FHuF4m4ehoT4IjHjU61eeT3FWhIUbnZhKd5DYgcjS8kOsd/XLq5/Vk+l+mityGO0KPP1dfChpQZzwOuBANpgIuRIOZBAIEQYEAQVAgQBAUCBAEBQIEQYEAQVAgQBAUXIieQ5wm417DpFkFF6JHRJlNfZPpmXN/NhTNvQ/agzj/s7RDgCkEs+Q/IyDANDJb8tVrJztlmS7Ne+wpHk52drVd/4A2jUFA1nUo7PX8bT8lIMBUxrHkE+VKdarzlAk6Xdr2rVypQJ0vyqpfWe3Q/pe4roONslU6lOtF27a6sT4lIMABjGPJly5nPUBQc4eoxuaSu3NtryJnOxKxvG0XJPnrPiFgyZ+gJd+ZF8RyXw+OZL37zfv2FMFlmD6wsKPO9nwQvO/Q1VJRF6YnFYC7zznre8jLKJyaP91Ioclt/uSDYZHOxQFAfFDZHfrlgaJesw+OeDvOITOT4DLMOEzVkq+JSi+V5T6O0k8LCHAopmvJl1NEvKnoEbCIkC2xrF+TgvxmPlPI0rzPGkHPE0jBYCogBYOZBAIEQYEAQVAgQBAUCBAEBQIEQYEAQVAgQBAUCBAEBQIEQYEAQVAgQBAUCBAE5a/Xr1/DDQOCgQgIggIBgqBAgCAoECAICgQIggIBgqBAgCAoECAICgQIggIBgqBAgCAoECAICgQIAkL0fwhZmvgjxPSSAAAAAElFTkSuQmCC)\n",
        "\n",
        "Al tener un acomodo distinto, se tenia que ir imagen por imagen agrupandola por su mascara para ser capaces de agregar el etiquetado teniendo (tensor_imagen,tensor_mask,label), se aplico tambien una interpolacion ya que al aplicar Resize en el caso de las mascaras, la segmentacion debe ser muy precisa ya que cierto numero en el pixel corresponde a determinada caracteristica por ejemplo fondo=0, la interpolacion al mas cercano hace que se omita la introduccion de otros valores que pueda afectar a la segmentacion ya realizada"
      ],
      "metadata": {
        "id": "H-OZ2nl3_5zK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CDataset(Dataset):\n",
        "    def __init__(self,dirlist):\n",
        "        self.samples = []\n",
        "        self.label_to_index = {\"COVID-19\": 0, \"Non-COVID\": 1, \"Normal\": 2} #Encoding labels haciendo mapeo\n",
        "        self.transform = {\n",
        "            'image': transforms.Compose([\n",
        "                transforms.Resize([256,256]),\n",
        "                transforms.ToTensor(),\n",
        "            ]),\n",
        "            'mask': transforms.Compose([\n",
        "                transforms.Resize([256,256],interpolation=transforms.InterpolationMode.NEAREST),#Aplicas interpolacion a las mascaras para no meter ruido, ya que en estas imagenes se asigna un numero a determinada caracteristica y aplicar cualquier interpolacion puede hacer que se interprete como un valor que originalmente no era\n",
        "                transforms.ToTensor(),\n",
        "            ])\n",
        "        }\n",
        "        for imagedirs, maskdirs, label in dirlist:\n",
        "            images = sorted(os.listdir(imagedirs))\n",
        "            for img in images:\n",
        "                self.samples.append((os.path.join(imagedirs, img), os.path.join(maskdirs, img), label))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path, label_ = self.samples[idx]\n",
        "        image = self.transform['image'](Image.open(img_path))\n",
        "        mask = self.transform['mask'](Image.open(mask_path))\n",
        "        label = self.label_to_index[label_]\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "        return image, mask, label_tensor"
      ],
      "metadata": {
        "id": "yjklISOk40tp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despues de crear el Dataset se pasa ese dataset al DataLoader, para asi poder agregar un batch size y facilitar la iteracion sobre el conjunto de datos de entrenamiento"
      ],
      "metadata": {
        "id": "o2gLsgCNAKtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainpath = \"/content/m3ex02-data/Train\"\n",
        "trainlist= [(trainpath+\"/COVID-19/images\",trainpath+\"/COVID-19/lung masks\",\"COVID-19\"),(trainpath+\"/Non-COVID/images\",trainpath+\"/Non-COVID/lung masks\",\"Non-COVID\"),(trainpath+\"/Normal/images\",trainpath+\"/Normal/lung masks\",\"Normal\")]\n",
        "valpath = \"/content/m3ex02-data/Val\"\n",
        "vallist= [(valpath+\"/COVID-19/images\",valpath+\"/COVID-19/lung masks\",\"COVID-19\"),(valpath+\"/Non-COVID/images\",valpath+\"/Non-COVID/lung masks\",\"Non-COVID\"),(valpath+\"/Normal/images\",valpath+\"/Normal/lung masks\",\"Normal\")]\n",
        "testpath = \"/content/m3ex02-data/Test\"\n",
        "testlist= [(testpath+\"/COVID-19/images\",testpath+\"/COVID-19/lung masks\",\"COVID-19\"),(testpath+\"/Non-COVID/images\",testpath+\"/Non-COVID/lung masks\",\"Non-COVID\"),(testpath+\"/Normal/images\",testpath+\"/Normal/lung masks\",\"Normal\")]\n",
        "dloadertrain = torch.utils.data.DataLoader(dataset=CDataset(trainlist), batch_size=30, shuffle=True, num_workers=0)\n",
        "dloadertest = torch.utils.data.DataLoader(dataset=CDataset(testlist), batch_size=30, shuffle=True, num_workers=0)\n",
        "dloaderval = torch.utils.data.DataLoader(dataset=CDataset(vallist), batch_size=30, shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "eqvhsM-S46uM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el modelo se hizo un Resnet con un Unet, sin usar el Unet preentrenado porque me dio muchos problemas, principalmente porque estaba entrenado con imagenes a color y en este caso estan en escalas de grises a un canal  \n",
        "Para poder utilizar el Resnet50 tuve que modificar la primera capa ya que iba a recibir imagenes en escala de grises  \n",
        "Para poder unir el Resnet50 con el Unet se debe tomar el primero como encoder y el segundo como decoder  \n",
        "Unet toma las caracteristicas extraidas de Resnet (resnet_features) y las procesa mediante capas de convolucion transpuesta  \n",
        "Se retorna tanto la mascara como el label  "
      ],
      "metadata": {
        "id": "FY2oD-6XAOAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetUnet(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "      #En este caso si se quiere combinar 2 modelos se debe usar uno como encoder y otro como decoder\n",
        "      #Resnet18\n",
        "      super().__init__()\n",
        "      self.resnet = models.resnet50(pretrained=True) #inicializando el modelo preentrenado\n",
        "      self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) # Modificamos la primera capa para que acepte imagenes en escalad e grises\n",
        "      self.resnet_features = nn.Sequential(*list(self.resnet.children())[:-2]) #Tomamos los features pero se quita la capa de salida para que podamos agregar unet como utilma capa y poder hacer la combinacion\n",
        "      # Al tener imagenes en escala de grises no utilice el unet preentrenado sin embargo se agreaga un unet simplificado\n",
        "      self.upconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2048, 1024, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "      self.conv_last = nn.Conv2d(64, 1, 1)\n",
        "      self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "      self.fc = nn.Linear(64, 3)\n",
        "  def forward(self, x):\n",
        "        # Extraer características con ResNet usandolo como encoder\n",
        "        features = self.resnet_features(x)\n",
        "        # Pasar las características a través de U-Net y usandolo como decorder\n",
        "        upconvfeatures = self.upconv(features)\n",
        "        mask  = nn.Sigmoid()(self.conv_last(upconvfeatures))\n",
        "\n",
        "        pool = self.avgpool(upconvfeatures)\n",
        "        flatten = torch.flatten(pool, 1)\n",
        "        label = nn.Softmax(dim=1)(self.fc(flatten))\n",
        "        return mask,label"
      ],
      "metadata": {
        "id": "0SqLAsE96Klx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta parte final se entrena, valida y prueba el codigo, se usan 2 Criterion ya que tenemos segmentacion y validacion y para sacar la perdida total se suman ambos"
      ],
      "metadata": {
        "id": "asnMRsv_ARZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la funcion para evaluar, validar y probar el modelo\n",
        "def fit(train,test,val,model,criterions,criterionc,optimizer,epochs):\n",
        "    #Al ser segmentacion y clasificacion se calcularan diferentes metricas\n",
        "    model.to(\"cuda\")\n",
        "    for epoch in range(epochs):\n",
        "        #Entrenamiento\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_accuracy = []\n",
        "        for images,masks,labels in train:\n",
        "            images,masks,labels = images.to(\"cuda\"),masks.to(\"cuda\"),labels.to(\"cuda\")\n",
        "            optimizer.zero_grad()\n",
        "            maskout,labelout = model(images)\n",
        "            loss = criterions(maskout, masks) + criterionc(labelout,labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "            _, predicted_labels = torch.max(labelout, 1)\n",
        "            accuracy = (predicted_labels == labels).float().mean()\n",
        "            train_accuracy.append(accuracy)\n",
        "        # Validación\n",
        "        val_loss = 0.0\n",
        "        val_accuracy = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for valimages,valmask, vallabels in val:\n",
        "                valimages,valmask, vallabels = valimages.to(\"cuda\"),valmask.to(\"cuda\"), vallabels.to(\"cuda\")\n",
        "                valmaskout,vallabelout = model(valimages)\n",
        "                val_loss += criterions(valmaskout, valmask).item() + criterionc(vallabelout,vallabels).item()\n",
        "                _, predicted_labels = torch.max(vallabelout, 1)\n",
        "                accuracyval = (predicted_labels == vallabels).float().mean()\n",
        "                val_accuracy.append(accuracyval)\n",
        "\n",
        "        print(f'Epoch {epoch+1}, Train Loss: {train_loss / len(train)}, Train Accuracy: {sum(train_accuracy) / len(train_accuracy)}, Val Loss: {val_loss / len(val)}, Val Accuracy: {sum(val_accuracy) / len(val_accuracy)}')\n",
        "\n",
        "    # Test\n",
        "    model.eval()\n",
        "    test_accuracy = []\n",
        "    with torch.no_grad():\n",
        "        for testimages,testmask, testlabels in test:\n",
        "            testimages,testmask, testlabels =testimages.to(\"cuda\"),testmask.to(\"cuda\"), testlabels.to(\"cuda\")\n",
        "            _,testlabelout = model(testimages)\n",
        "            _,predicted_labels = torch.max(testlabelout, 1)\n",
        "            testaccuracy = (predicted_labels == testlabels).float().mean()\n",
        "            test_accuracy.append(testaccuracy)\n",
        "\n",
        "    print(f'Test Accuracy: {sum(test_accuracy) / len(test_accuracy)}')\n",
        "\n",
        "\n",
        "model = ResnetUnet()\n",
        "criterions = nn.BCELoss()\n",
        "criterionc = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "fit(dloadertrain,dloadertest,dloaderval,model,criterions,criterionc,optimizer,10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwPgvYrF6OJq",
        "outputId": "98f6384a-6215-4e03-8a4b-813ce05e741f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 156MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 1.2313696600455606, Train Accuracy: 0.49755099415779114, Val Loss: 1.076038876291138, Val Accuracy: 0.5894482135772705\n",
            "Epoch 2, Train Loss: 1.0810796645629472, Train Accuracy: 0.5838956236839294, Val Loss: 1.077518243404383, Val Accuracy: 0.5869999527931213\n",
            "Epoch 3, Train Loss: 1.049110832052995, Train Accuracy: 0.6170908808708191, Val Loss: 0.9884608463687792, Val Accuracy: 0.669179379940033\n",
            "Epoch 4, Train Loss: 1.0688229524298927, Train Accuracy: 0.5753875970840454, Val Loss: 1.0786592377071882, Val Accuracy: 0.5497886538505554\n",
            "Epoch 5, Train Loss: 0.957478167273063, Train Accuracy: 0.69667649269104, Val Loss: 1.0150091676092938, Val Accuracy: 0.6451951861381531\n",
            "Epoch 6, Train Loss: 0.9139798278637354, Train Accuracy: 0.7356170415878296, Val Loss: 1.007282887415662, Val Accuracy: 0.6539914011955261\n",
            "Epoch 7, Train Loss: 0.9009072819302754, Train Accuracy: 0.7437660694122314, Val Loss: 0.9084743463449715, Val Accuracy: 0.7353049516677856\n",
            "Epoch 8, Train Loss: 0.8834115932000934, Train Accuracy: 0.7578637003898621, Val Loss: 0.8718369126072905, Val Accuracy: 0.7691478133201599\n",
            "Epoch 9, Train Loss: 0.8752079406825218, Train Accuracy: 0.7646497488021851, Val Loss: 0.871991612428789, Val Accuracy: 0.7639369368553162\n",
            "Epoch 10, Train Loss: 0.8651740851487902, Train Accuracy: 0.7747142910957336, Val Loss: 0.8431687274772818, Val Accuracy: 0.7882030606269836\n",
            "Test Accuracy: 0.8109032511711121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "savepath = \"/content/drive/MyDrive/Colab Notebooks/ExamenKarlaGarcia/model_resunet.pth\"\n",
        "torch.save(model.state_dict(), savepath)"
      ],
      "metadata": {
        "id": "Mf7O81d0HPdB"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}